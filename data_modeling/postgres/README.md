This project is to create a database schema and ETL pipeline for Sparkify, to enable it to analyze the data they've been collecting on songs and user activity on their new music streaming app.

### Project Structure 
`test.ipynb` displays the first few rows of each table to let you check your database.
`create_tables.py` drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
`etl.ipynb` reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
`etl.py` reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
`sql_queries.py` contains all your sql queries, and is imported into the last three files above.
README.md provides discussion on your project.

--------------------------------------------
### Datasets
- Song Dataset: A subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
- Log Dataset: Consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. 

--------------------------------------------
### Tables and Schema
**Fact Table**
- songplays - records in log data associated with song plays i.e. records with page NextSong
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

**Dimension Tables**
- users - users in the app
- songs - songs in music database
- artists - artists in music database
- time - timestamps of records in songplays broken down into specific units

--------------------------------------------
### Running 
- In the terminal, run `python create_tables.py` to create the tables
- Once the tables are created, run `python etl.py`
- To test queries, the notebook `test.ipynb` can be used

